{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sqrt, sum, abs, max, maximum, logspace, exp, log, log10, zeros\n",
    "from numpy.random import normal, randn, choice\n",
    "from numpy.linalg import norm\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.linalg import orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For logistic regression\n",
    "\n",
    "def buildmat(m,n,cond_number):\n",
    "    \"\"\"Build an mxn matrix with condition number cond.\"\"\"\n",
    "    if m<=n:\n",
    "        U = randn(m,m);\n",
    "        U = orth(U);\n",
    "        Vt = randn(n, m);\n",
    "        Vt = orth(Vt).T;\n",
    "        S = 1/logspace(0,log10(cond_number),num=m);\n",
    "        return (U*S[:,None]).dot(Vt)\n",
    "    else:\n",
    "        return buildmat(n,m,cond_number).T\n",
    "    \n",
    "def create_classification_problem(num_data, num_features, cond_number):\n",
    "    \"\"\"Build a simple classification problem.\"\"\"\n",
    "    X = buildmat(num_data, num_features, cond_number)\n",
    "    # The linear dividing line between the classes\n",
    "    w =  randn(num_features,1)\n",
    "    # create labels\n",
    "    prods = X@w\n",
    "    y = np.sign(prods)\n",
    "    #  mess up the labels on 10% of data\n",
    "    flip = choice(range(num_data),int(num_data/10))\n",
    "    y[flip] = -y[flip]\n",
    "    #  return result\n",
    "    return X,y\n",
    "\n",
    "def logistic_loss(z):\n",
    "    \"\"\"Return sum(log(1+exp(-z))). Your implementation can NEVER exponentiate a positive number.  No for loops.\"\"\"\n",
    "    loss = zeros(z.shape)\n",
    "    loss[z>=0] = log(1+exp(-z[z>=0]))\n",
    "    # Make sure we only evaluate exponential on negative numbers\n",
    "    loss[z<0] = -z[z<0]+log(1+exp(z[z<0]))\n",
    "    return np.sum(loss)\n",
    "\n",
    "def logreg_objective(w,X,y):\n",
    "    \"\"\"Evaluate the logistic regression loss function on the data and labels, where the rows of D contain \n",
    "    feature vectors, and y is a 1D vector of +1/-1 labels.\"\"\"\n",
    "    z = y*(X@w)\n",
    "    return logistic_loss(z)\n",
    "\n",
    "def logistic_loss_grad(z):\n",
    "    \"\"\"Gradient of logistic loss\"\"\"\n",
    "    grad = zeros(z.shape)\n",
    "    neg = z.ravel() <=0\n",
    "    pos = z.ravel() > 0\n",
    "    grad[neg] = -1/(1+exp(z[neg]))\n",
    "    grad[pos] = -exp(-z[pos])/(1+exp(-z[pos]))\n",
    "    return grad\n",
    "\n",
    "def logreg_objective_grad(w,X,y):\n",
    "    return X.T@(y*logistic_loss_grad(y*X@w))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
